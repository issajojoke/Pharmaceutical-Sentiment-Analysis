{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt') # takes abbreviations into account\n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "import tensorflow\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataframe\n",
    "df = pd.read_csv('balanced_df.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('drugs_clean_train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "df['stemmed'] = df['stemmed'].apply(ast.literal_eval)  # Safely evaluate string as a list\n",
    "df['stemmed'] = df['stemmed'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['stemmed']\n",
    "y = df['class']\n",
    "\n",
    "# split data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=1000,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,8),max_features=2000)\n",
    "tfidf_vec.fit(x)\n",
    "\n",
    "x_train_tfidf = tfidf_vec.transform(x_train)\n",
    "x_test_tfidf = tfidf_vec.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf_vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbc = naive_bayes.MultinomialNB()\n",
    "nbc.fit(x_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_nbc = nbc.predict(x_test_tfidf)\n",
    "print(f'NBC accuracy: {accuracy_score(predict_nbc,y_test)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(x_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_svc = clf.predict(x_test_tfidf)\n",
    "print(f'SVC accuracy: {accuracy_score(predict_svc,y_test)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras/Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df['stemmed']\n",
    "b = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "vocab_size = 2000\n",
    "max_len = 50\n",
    "\n",
    "# Tokenizer setup\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(df['stemmed'])\n",
    "\n",
    "a_sequences = tokenizer.texts_to_sequences(df['stemmed']) # token to sequence\n",
    "a_padded = pad_sequences(a_sequences, maxlen=max_len, padding='post', truncating='post') # input size\n",
    "\n",
    "b_encoded = df['class'].values - 1\n",
    "\n",
    "a_train, a_test, b_train, b_test = train_test_split(a_padded, b_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "num_feature = a_padded.shape[1] # input sequence\n",
    "num_classes = len(np.unique(b_encoded)) # classes\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(num_feature,)),\n",
    "    Dense(num_classes, activation='softmax')  # Softmax for multi-class classification\n",
    "])\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(a_train, b_train, validation_data=(a_test, b_test), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(a_test, b_test)\n",
    "print(\"Validation Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize history for accuracy\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Summarize history for loss\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accounts for context and nuances in language "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import torch\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def batch(start_index,end_index,input_ids,attention_mask,model):\n",
    "    batch_input_ids = input_ids[start_index:end_index].to(device)\n",
    "    batch_attention_mask = attention_mask[start_index:end_index].to(device)\n",
    "    batch_inputs = {'input_ids': batch_input_ids, 'attention_mask': batch_attention_mask}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**batch_inputs)\n",
    "    return output.logits.cpu()\n",
    "\n",
    "text = df['review']\n",
    "encoded_input = tokenizer(text.tolist(), return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "\n",
    "batchsize = 10\n",
    "input_ids = encoded_input['input_ids']\n",
    "attention_mask = encoded_input['attention_mask']\n",
    "\n",
    "total_batch = (len(input_ids) + batchsize - 1)//batchsize\n",
    "\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(batch)(i * batchsize, min((i + 1) * batchsize, len(input_ids)), input_ids, attention_mask, model)\n",
    "    for i in range(total_batch))\n",
    "\n",
    "all_scores = torch.cat(results,dim=0)\n",
    "print(all_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code above took 483.46 mins to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() \n",
    "\n",
    "X_train = scaler.fit_transform(all_scores.numpy())\n",
    "Y_train = df['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save bert scores to avoid running it again\n",
    "torch.save(all_scores, 'bert_scores.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to dataframe\n",
    "scores_list = [scores.tolist() for scores in all_scores]\n",
    "df['distilbert_scores'] = scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# convert from tensor to numpy array\n",
    "logits_numpy = all_scores.numpy()\n",
    "\n",
    "# scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(logits_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = all_scores\n",
    "Y = df['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Model with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = gnb.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM model with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='rbf')\n",
    "svc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = svc.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network Model with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = all_scores.numpy()\n",
    "b = df['class']\n",
    "\n",
    "#split\n",
    "a_train,a_test,b_train,b_test = train_test_split(a,b,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_train -= 1\n",
    "b_test -= 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "num_feat = a.shape[1] # from distilbert\n",
    "num_class = len(np.unique(np.concatenate([b_train, b_test]))) # all ratings\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(num_feat,)),\n",
    "    Dense(num_class, activation='softmax') # softmax for multi-class classification\n",
    "])\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "history = model.fit(a_train,b_train,validation_data=(a_test,b_test),epochs=10,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(a_test, b_test)\n",
    "print(\"Validation Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
